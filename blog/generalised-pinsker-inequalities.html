<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>

   <!-- Standard site meta information -->
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>Generalised Pinsker Inequalities and Surrogate Regret Bounds ← Inductio Ex Machina ← Mark Reid</title>
   <meta name="author" content="Mark Reid" />

   <link rel="openid.server" href="http://www.myopenid.com/server" />
   <link rel="openid.delegate" href="http://mark.reid.name" />

   <link rel="start" href="../" />

   <!-- syntax highlighting CSS -->
   <link rel="stylesheet" href="../css/syntax.css" type="text/css" />

   <!-- Homepage CSS -->
   <link rel="stylesheet" href="../css/screen.css" type="text/css" media="all" />
   <link rel="stylesheet" href="../css/print.css" type="text/css" media="print" />
   <link rel="stylesheet" href="../css/widgets.css" type="text/css" media="all" />
   <link rel="stylesheet" href="../css/amjr.css" type="text/css" media="all" />

 	<meta name="twitter:card" content="summary">
    <meta name="twitter:creator" content="@mdreid">
    <meta name="twitter:site" content="@mdreid">
    <meta name="twitter:title" content="Generalised Pinsker Inequalities and Surrogate Regret Bounds">
    <meta name="twitter:description" content="Robert Williamson and I have had two papers accepted at ICML and COLT 2009. They are both about bounds -- one for surrogate losses the other for f-divergences.">

<!-- MathJax configuration and loading -->
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		extensions: ["tex2jax.js"],
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["\\(","\\)"] ],
			displayMath: [ ["\\[","\\]"] ],
			processEscapes: true
		},
		TeX: { equationNumbers: { autoNumber: "AMS" } },
		"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"}> </script>

</head>

<body id="Blog">

<div id="site">
  <div id="header">
<h1>
	<a href="../blog/" title="A machine learning blog">Inductio ex Machina</a>
	<span class="byline">← <a href="../">Mark Reid</a></span>
</h1>
<ul class="nav">
  <li><a class="home" href="../blog/">Home</a></li>
  <li><a class="info" href="../blog/info.html">Info</a></li>
  <li><a class="past" href="../blog/past.html">Past</a></li>
  <li><a class="kith" href="../blog/kith.html">Kith</a></li>
</ul>
</div>

<div id="page">

<h1 class="emphnext">Generalised Pinsker Inequalities and Surrogate Regret Bounds</h1>

<p>Just a quick note to say that <a href="http://axiom.anu.edu.au/~williams/">Bob Williamson</a> and I have improved a few of the results we had previously written up in our <a href="../iem/information-divergence-and-risk.html">technical report</a> and even managed to get some of them accepted to <a href="http://www.cs.mcgill.ca/~icml2009/">ICML</a> and <a href="http://www.cs.mcgill.ca/~colt2009">COLT</a> this year.</p>
<p>The ICML paper is <a href="http://www.cs.mcgill.ca/~icml2009/abstracts.html#400">Surrogate Regret Bounds for Proper Losses</a>. In it we extend earlier results on surrogate bounds for 0–1 misclassification loss to cost-weighted misclassification losses and proper losses. We also investigate conditions for convexity of composite proper losses.</p>
<p>As the abstract puts it:</p>
<blockquote>
<p>We present tight surrogate regret bounds for the class of proper (i.e., Fisher consistent) losses. The bounds generalise the margin-based bounds due to Bartlett et al. (2006). The proof uses Taylor’s theorem and leads to new representations for loss and regret and a simple proof of the integral representation of proper losses. We also present a different formulation of a duality result of Bregman divergences which leads to a demonstration of the convexity of composite losses using canonical link functions.</p>
</blockquote>
<p>What I particularly like about this paper is that these results and several earlier ones regarding proper losses are arrived at using little more than a Taylor series expansion.</p>
<p>The COLT paper, <a href="../bits/pubs/colt09.pdf">Generalised Pinsker Inequalities</a>, is (unsurprisingly) a bit more technical. As the name suggests, we were able to generalise the classical <a href="http://en.wikipedia.org/wiki/Pinsker's_inequality">Pinsker inequality</a><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> that provides a lower bound on the KL divergence <span class="math">\(KL(P,Q)\)</span> between the distributions <span class="math">\(P\)</span> and <span class="math">\(Q\)</span> in terms of their variational divergence <span class="math">\(V(P,Q)\)</span>: <span class="math">\[
	KL(P,Q) \ge \frac{1}{2} [V(P,Q)]^2 
\]</span></p>
<p>Not only were we able to tighten this bound but we provide a template for constructing similar (and tight) bounds for any <a href="http://en.wikipedia.org/wiki/F-divergence">f-divergence</a> in terms of variational divergence. Furthermore, we also generalise the bounds to make use of one or more non-symmetric variational divergences.</p>
<p>Here’s the abstract:</p>
<blockquote>
<p>We generalise the classical Pinsker inequality which relates variational divergence to Kullback-Liebler divergence in two ways: we consider arbitrary f -divergences in place of KL divergence, and we assume knowledge of a sequence of values of generalised variational divergences. We then develop a best possible inequality for this doubly generalised situation. Specialising our result to the classical case provides a new and tight explicit bound relating KL to variational divergence (solving a problem posed by Vajda some 40 years ago). The solution relies on exploiting a connection between divergences and the Bayes risk of a learning problem via an integral representation.</p>
</blockquote>
<p>Although the proof requires some careful, technical detail, the approach is very simple and geometric. It makes use of the fact that any f-divergence can be written as a weighted integral of variational-like divergences and the fact that f-divergences are related to Bayes risk curves for proper losses.</p>
<p>In a very loose sense, these papers are duals of each other: the bounds in both share a similar structure — what can be said about a “complex” divergence given information about some “simple” divergences vs. what can be said about a “simple” loss given information about a “complex” one — and both are driven by the integral representation of losses and divergences which provides an interpretation of “simple” and “complex”.</p>
<p>I’ll be presenting both these papers in Montréal in June so if you are also attending ICML and COLT and are interested in these results please let me know.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The statement of Pinsker’s inequality on the Wikipedia page uses a version of variational divergence that differs from ours by a factor of 2.<a href="#fnref1">↩</a></p></li>
</ol>
</div>


<address class="signature">
	<a class="author" href="http://mark.reid.name">Mark Reid</a>
  <span class="date">May 23, 2009</span>
  <span class="location">Canberra, Australia</span>
</address>

</div>
<address id="feed" class="quiet right">Subscribe: <a href="../blog/atom.xml" title="Subscribe to Atom feed"><em>Atom Feed</em></a></address>


<!-- Disqus Comments -->
<div id="disqus_thread"></div>

<!-- Enable Disqus comments -->
<script type="text/javascript">
        var disqus_iframe_css = "http://mark.reid.name/css/screen.css";
        var disqus_title = "Generalised Pinsker Inequalities and Surrogate Regret Bounds";
        var disqus_message = "Robert Williamson and I have had two papers accepted at ICML and COLT 2009. They are both about bounds -- one for surrogate losses the other for f-divergences.";
</script>
<script type="text/javascript" src="http://disqus.com/forums/markreid/embed.js"></script>

<noscript>
    <a href="http://markreid.disqus.com/?url=ref">View the discussion thread.</a>
</noscript>




    <!-- Footer with copyright and contact information -->
  <div id="footer">
	<address>
		<span class="copyright">
			Content &amp; Design by 
			<a href="../info/site.html">Mark Reid</a>
			<br />
			(<a rel="licence" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Some rights reserved</a>)			
		</span>
		<span class="engine">
			Powered by 
			<a href="http://jaspervdj.be/hakyll/" title="A static, minimalist, Haskell-powered CMS">Hakyll</a>
		</span>
	</address>
  </div>

</div>


<!-- Google Analytics script -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-1051817-4");
pageTracker._trackPageview();
</script>
<!--[if IE 6]>
<script type="text/javascript"> 
	/*Load jQuery if not already loaded*/ if(typeof jQuery == 'undefined'){ document.write("<script type=\"text/javascript\"   src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js\"></"+"script>"); var __noconflict = true; } 
	var IE6UPDATE_OPTIONS = {
		icons_path: "http://static.ie6update.com/hosted/ie6update/images/"
	}
</script>
<script type="text/javascript" src="http://static.ie6update.com/hosted/ie6update/ie6update.js"></script>
<![endif]-->


</body>
</html>



