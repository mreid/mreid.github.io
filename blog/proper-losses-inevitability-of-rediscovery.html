<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>

   <!-- Standard site meta information -->
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>Proper Losses and the Inevitability of Rediscovery ← Inductio Ex Machina ← Mark Reid</title>
   <meta name="author" content="Mark Reid" />

   <link rel="openid.server" href="http://www.myopenid.com/server" />
   <link rel="openid.delegate" href="http://mark.reid.name" />

   <link rel="start" href="../" />

   <!-- syntax highlighting CSS -->
   <link rel="stylesheet" href="../css/syntax.css" type="text/css" />

   <!-- Homepage CSS -->
   <link rel="stylesheet" href="../css/screen.css" type="text/css" media="all" />
   <link rel="stylesheet" href="../css/print.css" type="text/css" media="print" />
   <link rel="stylesheet" href="../css/widgets.css" type="text/css" media="all" />
   <link rel="stylesheet" href="../css/amjr.css" type="text/css" media="all" />

 	<meta name="twitter:card" content="summary">
    <meta name="twitter:creator" content="@mdreid">
    <meta name="twitter:site" content="@mdreid">
    <meta name="twitter:title" content="Proper Losses and the Inevitability of Rediscovery">
    <meta name="twitter:description" content="I recently discovered that a result concerning probability estimation in one of my recent papers was already observed by Lindley 28 years prior.">

<!-- MathJax configuration and loading -->
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		extensions: ["tex2jax.js"],
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["\\(","\\)"] ],
			displayMath: [ ["\\[","\\]"] ],
			processEscapes: true
		},
		TeX: { equationNumbers: { autoNumber: "AMS" } },
		"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"}> </script>

</head>

<body id="Blog">

<div id="site">
  <div id="header">
<h1>
	<a href="../blog/" title="A machine learning blog">Inductio ex Machina</a>
	<span class="byline">← <a href="../">Mark Reid</a></span>
</h1>
<ul class="nav">
  <li><a class="home" href="../blog/">Home</a></li>
  <li><a class="info" href="../blog/info.html">Info</a></li>
  <li><a class="past" href="../blog/past.html">Past</a></li>
  <li><a class="kith" href="../blog/kith.html">Kith</a></li>
</ul>
</div>

<div id="page">

<h1 class="emphnext">Proper Losses and the Inevitability of Rediscovery</h1>

<p>Suppose you have trained a real-valued binary predictor <span class="math">\(h\)</span> using some learning algorithm. When is it reasonable to relate the real number <span class="math">\(h(x)\)</span> to the probability <span class="math">\(P(y=1|x)\)</span>, that is, the probability of <span class="math">\(x\)</span> having a positive label?</p>
<p>While there have been quite a few proposed answers to this question, they tend to focus on providing answers for specific techniques, showing how to modify SVM or boosting algorithms or their outputs to obtain probabilities<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. In our recent paper, <em><a href="http://jmlr.csail.mit.edu/papers/v11/reid10a.html">Composite Binary Losses</a></em>, <a href="http://users.cecs.anu.edu.au/~williams/">Bob Williamson</a> and I took a more problem-oriented look at this question (amongst others). Rather than looking at specific techniques, we focused on the loss function that a learner attempts to minimise. By exploiting existing results about <a href="http://mark.reid.name/iem/proper-losses.html">proper losses</a> we were able to characterise which losses allow the output of a classifier to be transformed into consistent probability estimates.</p>
<h2 id="proper-composite-losses">Proper Composite Losses</h2>
<p>A loss for probability estimation <span class="math">\(\ell\)</span> is one that assigns a penalty <span class="math">\(\ell(y,q)\)</span> when <span class="math">\(q\)</span> is the estimate of the probability that some <span class="math">\(x\)</span> has label 1 when <span class="math">\(x\)</span> is actually classified as <span class="math">\(y \in \{-1,1\}\)</span>. For example, square loss has <span class="math">\(\ell(1,q) = (1-q)^2\)</span> and <span class="math">\(\ell(0,q) = q^2\)</span>. A <em>proper loss</em> additionally requires that its conditional expected loss <span class="math">\(L(p,q) := \mathbb{E}_{y\sim p} [\ell(y,q)]\)</span> is minimised when <span class="math">\(q=p\)</span>, that is, if the true conditional probability of <span class="math">\(y=1\)</span> is estimated correctly.</p>
<p>De Finetti, McCarthy, and Savage<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> were among the first to observe that this innocuous little condition was the “right” one to ask of losses for probability estimation. That being the case, the original question of interpreting the real-valued output of a predictor can be reduced to asking when there exists an inveritble function <span class="math">\(\psi^{-1}\)</span> that maps real numbers into the interval <span class="math">\([0,1]\)</span>. If such a function exists, we can then ask when a loss <span class="math">\(\lambda(y,v)\)</span> for real-valued outputs <span class="math">\(v\)</span> can be expressed as the composition of a proper loss <span class="math">\(\ell\)</span> and the mapping <span class="math">\(\psi^{-1}\)</span>. That is, when can we write <span class="math">\(\lambda(y,v) = \ell(y, \psi^{-1}(v))\)</span> for some proper loss <span class="math">\(\ell\)</span>?</p>
<p>Corollary 12 in my paper with Bob states that a suitably differentiable <span class="math">\(\lambda\)</span> can the decomposed in this way if and only if <span class="math">\[ \displaystyle 
	\psi^{-1}(v) = \frac{\lambda'_{-1}(v)}{\lambda'_{-1}(v) - \lambda'_1(v)}.
\]</span> where <span class="math">\(\lambda_y(v)\)</span> is shorthand for <span class="math">\(\lambda(y,v)\)</span>.</p>
<p>This very simple characterisation means you can do a simple calculation to determine whether or not your trained classifier can give consistent probability estimates or not. If your classifier <span class="math">\(h\)</span> was trained by minimising a loss which makes the above equation is well-defined then you can interpret <span class="math">\(\psi^{-1}(h(x))\)</span> as a probability, otherwise you cannot. For example, the exponential loss minimsed by AdaBoost does satisfy the above condition, while the hinge loss used by SVMs does not.</p>
<h2 id="lindleys-lemma">Lindley’s Lemma</h2>
<p>The proof of the above result follows almost directly from the definition of composite losses and properness. I am always a little suspicious when an answer to a fairly fundamental question is so easily obtained.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> In this case my suspicions were justified since we were not the first to state this result: <a href="http://en.wikipedia.org/wiki/Dennis_Lindley">Dennis Lindley</a> had also noticed this in his paper from 1982, <em><a href="http://www.jstor.org/stable/1402448">Scoring Rules and the Inevitability of Probability</a></em>.</p>
<p>His Lemma 1 can be paraphrased as:</p>
<blockquote>
<p>Given a <em>score function</em> <span class="math">\(f(X,E)\)</span> that satisfies some assumptions regarding its admissibility, origin and scale, regularity, it is possible to define a function <span class="math">\[ \displaystyle 
	P(x) = \frac{f'(x,0)}{f'(x,0) - f'(x,1)}
\]</span> for <span class="math">\(x\in[x_F, x_T]\)</span> such that <span class="math">\(P(x)\in[0,1]\)</span> is continuous and <span class="math">\(P(x_F)=0\)</span>, <span class="math">\(P(x_T)=1\)</span>.</p>
</blockquote>
<p>That should look familiar…</p>
<p>Given Lindley’s prominence and the fact the title of his paper contains “scoring rules”, I’m a little embarassed we hadn’t found his version of the result before we rediscovered it. Hopefully this post goes some way to addressing the oversight.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>For example, see the overview in John Platt’s <em><a href="http://research.microsoft.com/apps/pubs/default.aspx?id=69187">Probabilities for SV Machines</a></em>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Savage, <em><a href="http://www.jstor.org/pss/2284229">Elicitation of Personal Probabilities and Expectations</a></em>, JASA, 1971.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>I had a similar gut feeling that was justified when I was exploring <a href="http://mark.reid.name/iem/a-compression-lemma.html">the Compression Lemma</a>.<a href="#fnref3">↩</a></p></li>
</ol>
</div>


<address class="signature">
	<a class="author" href="http://mark.reid.name">Mark Reid</a>
  <span class="date">April 21, 2011</span>
  <span class="location">Darwin, Australia</span>
</address>

</div>
<address id="feed" class="quiet right">Subscribe: <a href="../blog/atom.xml" title="Subscribe to Atom feed"><em>Atom Feed</em></a></address>


<!-- Disqus Comments -->
<div id="disqus_thread"></div>

<!-- Enable Disqus comments -->
<script type="text/javascript">
        var disqus_iframe_css = "http://mark.reid.name/css/screen.css";
        var disqus_title = "Proper Losses and the Inevitability of Rediscovery";
        var disqus_message = "I recently discovered that a result concerning probability estimation in one of my recent papers was already observed by Lindley 28 years prior.";
</script>
<script type="text/javascript" src="http://disqus.com/forums/markreid/embed.js"></script>

<noscript>
    <a href="http://markreid.disqus.com/?url=ref">View the discussion thread.</a>
</noscript>




    <!-- Footer with copyright and contact information -->
  <div id="footer">
	<address>
		<span class="copyright">
			Content &amp; Design by 
			<a href="../info/site.html">Mark Reid</a>
			<br />
			(<a rel="licence" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Some rights reserved</a>)			
		</span>
		<span class="engine">
			Powered by 
			<a href="http://jaspervdj.be/hakyll/" title="A static, minimalist, Haskell-powered CMS">Hakyll</a>
		</span>
	</address>
  </div>

</div>


<!-- Google Analytics script -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-1051817-4");
pageTracker._trackPageview();
</script>
<!--[if IE 6]>
<script type="text/javascript"> 
	/*Load jQuery if not already loaded*/ if(typeof jQuery == 'undefined'){ document.write("<script type=\"text/javascript\"   src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js\"></"+"script>"); var __noconflict = true; } 
	var IE6UPDATE_OPTIONS = {
		icons_path: "http://static.ie6update.com/hosted/ie6update/images/"
	}
</script>
<script type="text/javascript" src="http://static.ie6update.com/hosted/ie6update/ie6update.js"></script>
<![endif]-->


</body>
</html>



