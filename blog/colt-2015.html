<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>

   <!-- Standard site meta information -->
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>COLT 2015 in Review ← Inductio Ex Machina ← Mark Reid</title>
   <meta name="author" content="Mark Reid" />

   <link rel="openid.server" href="http://www.myopenid.com/server" />
   <link rel="openid.delegate" href="http://mark.reid.name" />

   <link rel="start" href="../" />

   <!-- syntax highlighting CSS -->
   <link rel="stylesheet" href="../css/syntax.css" type="text/css" />

   <!-- Homepage CSS -->
   <link rel="stylesheet" href="../css/screen.css" type="text/css" media="all" />
   <link rel="stylesheet" href="../css/print.css" type="text/css" media="print" />
   <link rel="stylesheet" href="../css/widgets.css" type="text/css" media="all" />
   <link rel="stylesheet" href="../css/amjr.css" type="text/css" media="all" />

 	<meta name="twitter:card" content="summary">
    <meta name="twitter:creator" content="@mdreid">
    <meta name="twitter:site" content="@mdreid">
    <meta name="twitter:title" content="COLT 2015 in Review">
    <meta name="twitter:description" content="Despite the jet-lag and the extreme heat, I had a very enjoyable time at COLT this year. This is a summary of some of the highlights for me as well as a list of work I saw that I'd like to investigate further.">

<!-- MathJax configuration and loading -->
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		extensions: ["tex2jax.js"],
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["\\(","\\)"] ],
			displayMath: [ ["\\[","\\]"] ],
			processEscapes: true
		},
		TeX: { equationNumbers: { autoNumber: "AMS" } },
		"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"}> </script>

</head>

<body id="Blog">

<div id="site">
  <div id="header">
<h1>
	<a href="../blog/" title="A machine learning blog">Inductio ex Machina</a>
	<span class="byline">← <a href="../">Mark Reid</a></span>
</h1>
<ul class="nav">
  <li><a class="home" href="../blog/">Home</a></li>
  <li><a class="info" href="../blog/info.html">Info</a></li>
  <li><a class="past" href="../blog/past.html">Past</a></li>
  <li><a class="kith" href="../blog/kith.html">Kith</a></li>
</ul>
</div>

<div id="page">

<h1 class="emphnext">COLT 2015 in Review</h1>

<p>It’s been about three weeks since I jotted down some notes about <a href="http://www.learningtheory.org/colt2015/">COLT</a> this year while I was on the train from Paris to Lille (for ICML). I’ve finally made some time to polish them a little and put them up in continuing attempt to <a href="../blog/restarting.html">get back into blogging</a>.</p>
<p>COLT this year was held at the Jussieu campus of the <a href="http://www.upmc.fr/en/index.html">Université Pierre et Marie Curie</a> in the 5th arrondissement of Paris. I was fortunate to be staying a short walk away at the air-conditioned <a href="http://www.paris-hotel-des-nations-st-germain.com/en/home/?r=3955002">Hotels Des Nations Saint Germain</a> since the temperature was over 40ºC on the first few days. Apart from the slightly uncomfortable temperature though, this was a very hard conference to fault: the venue, talks, poster sessions, invited lectures, catering, and events were all excellent.</p>
<p>I’ve tried to capture some of the highlights below, as well as the parts of the <a href="http://easychair.org/smart-program/COLT2015/index.html">full program</a> that I saw and intend to follow up on.</p>
<h2 id="invited-talks">Invited Talks</h2>
<p>The talk by Fields medalist <a href="https://en.wikipedia.org/wiki/Cédric_Villani">Cédric Villani</a> on the <em>Synthetic Theory of Ricci Curvature</em> was a thought-provoking and entertaining highlight of COLT — at least the parts I was able to comprehend. He started his talk by explaining the distinction between <em>analytic</em> and <em>synthetic</em> theories by way of the example of convexity. The analytic take on convexity is what Villani called a “local” and “effective” theory: a function is convex if its Hessian is positive semi-definite. It’s local because the Hessian is defined using neighbourhoods of points and effective because often one can compute the and test the Hessian. The synthetic definition of a convex function is the <a href="../blog/behold-jensens-inequality.html">usual one</a> where the value at the average of two points be no more than the average of the values at those points. This, while typically harder to establish than the analytic definition, has the advantage of being easily generalised to non-differentiable functions and leads more directly to useful inequalities.</p>
<p>The rest of his talk I found a little more difficult to follow but from what I recall, he sketched out several definitions of positive <a href="https://en.wikipedia.org/wiki/Ricci_curvature">Ricci curvature</a>. In two dimensions it is the “correction” to the distance between two orthonormal vectors relative to Euclidean space or the expansion of the median of triangles due to the space; in three or more, the rate of change of a volume element along a geodesic. From there he listed the way this concept connected a variety of ideas and bounds from information theory and optimal transport.</p>
<p>It seems a large portion of the talk was taken from <a href="http://cedricvillani.org/wp-content/uploads/2015/07/takagi-2.pdf">notes</a> that were based on lectures he gave this year at Tsinghua University and ETH Zürich.</p>
<div class="figure">
<img src="../pics/villani.jpg" alt="Cédric Villani’s talk on Ricci curvature." /><p class="caption">Cédric Villani’s talk on Ricci curvature.</p>
</div>
<p>Slightly more down to earth, but no less engaging were <a href="http://www.cs.yale.edu/homes/spielman/">Daniel Spielman</a>’s and <a href="http://theory.stanford.edu/~tim/">Tim Roughgarden</a>’s talks.</p>
<p>Dan gave an excellent and intuitive introduction to Laplacians, their properties, and connections to finding solutions of special types of linear equations. He then went onto discuss some impressive results he and others developed in solving these systems using “<a href="http://arxiv.org/abs/0808.4134">sparsification</a>” of graphs associated with the Laplacian matrices. The resulting, almost linear-time algorithms will likely form the basis of many efficient techniques in machine learning, maximum flow problems, and PDEs. An overview of part of his talk can be found in <a href="http://www.cs.yale.edu/homes/spielman/PAPERS/icm10post.pdf">these notes</a>.</p>
<p>Tim gave a fascinating overview of how several ideas from learning theory, including no-regret learning and PAC-style analysis, have recently made their way into economics and game theory. One focus of the talk that caught my attention was his discussion of what he calls an “extension theorem” for <a href="https://en.wikipedia.org/wiki/Price_of_anarchy">price of anarchy</a> results.</p>
<p>Roughly speaking, price of anarchy results measure how inefficient multiagent games become when agents behave selfishly relative to a optimal, centrally coordinated plan (<em>e.g.</em>, routing traffic). These results were originally stated in terms of Nash equilibria which are known to be “fragile” solution concepts. More recently, <a href="http://theory.stanford.edu/~tim/papers/robust.pdf">more robust analyses</a> are possible by replacing the assumption that agents play their equilibrium strategy with a much weaker assumption that they engage in repeated play that generates no-regret outcome sequences. The striking thing about Tim’s extension theorem is that he shows how equilibrium-based price of anarchy proofs for a large class of “smooth” games can be automatically transformed into proofs for the weaker, no-regret versions.</p>
<p>It’s rare and exciting to see this type of “meta” theorem that applies to whole classes of existing results. To top it off, there seems to be a lot of scope and interest in developing more connections like these between economics, game theory, and machine learning.</p>
<h2 id="other-talks-posters">Other Talks &amp; Posters</h2>
<p>Although I wasn’t able to attend all of the regular sessions, I did see a lot of interesting stuff that I hope to catch up on now that I’m home. I think the format for the talks this year worked really well. There were a handful of carefully picked 20 minute talks and the rest of the speakers got 5 minutes to pique the interest of the audience enough to have them come to their posters.</p>
<p>Of the longer talks, I really enjoyed <a href="http://www.cs.berkeley.edu/~christos/">Christos Papadimitriou</a>’s one on his work with <a href="http://www.cc.gatech.edu/~vempala/">Santosh Vempala</a> on <em><a href="http://jmlr.org/proceedings/papers/v40/Papadimitriou15.html">Cortical Learning via Prediction</a></em> and <a href="http://www.princeton.edu/~sbubeck/">Sébastien Bubeck</a>’s very well delivered blackboard talk on <em><a href="http://jmlr.org/proceedings/papers/v40/Bubeck15b.html">The Entropic Barrier</a></em> with <a href="http://www.wisdom.weizmann.ac.il/~ronene/">Ronen Eldan</a> (<a href="http://arxiv.org/abs/1412.1587">arXiv preprint</a>).</p>
<p>From what I’ve understood of it so far, Christos and Santosh have built upon Leslie Valiant’s <a href="https://books.google.com.au/books/about/Circuits_of_the_mind.html?id=JqfwAAAAMAAJ">neuroidal model of the brain</a>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> They show that by introducing a new operation, caled PJOIN for “predictive join”, they are able to implement pattern recognition algorithms that do not suffer the combinatorial explosion that occurs if limited to the model’s original operations (JOIN &amp; LINK). I’m hoping to spend some time looking at this further and alongside some interesting recent work by <a href="https://sites.google.com/site/dbalduzzi/">David Balduzzi</a> on <a href="http://arxiv.org/pdf/1401.1465v1.pdf">Cortical Prediction Markets</a>. I’ve been thinking about <a href="http://arxiv.org/abs/1410.0413">networks of traders</a> with <a href="http://people.seas.harvard.edu/~raf/">Raf</a> recently and think these neurologically inspired takes on networks provide an interesting perspective.</p>
<p>Sébastien and Ronen’s work give a very natural construction of a <a href="https://en.wikipedia.org/wiki/Self-concordant_function">self-concordant barrier</a> for convex bodies. Given a compact convex body <span class="math">\(\mathcal{K} \subset \mathbb{R}^n\)</span> they define <span class="math">\(f\)</span> to be the log partition function for the exponential family of densities <span class="math">\(p_\theta\)</span> with natural parameters <span class="math">\(\theta \in \mathcal{K}\)</span> relative to the uniform distribution. The Fenchel dual, <span class="math">\(f^*(x) = -H(p_{\theta(x)})\)</span> with <span class="math">\(\theta(x) = \nabla f^*(x)\)</span> is then a <span class="math">\((1 + o(1))n\)</span>-self-concordant barrier for <span class="math">\(\mathcal{K}\)</span>. Using this elegant connection between barriers, exponential families, and duals, they are able to recover near-optimal bounds for online linear optimisation problems with bandit feedback.</p>
<p><a href="http://people.seas.harvard.edu/~raf/">Raf</a> and I have looked at <a href="http://mark.reid.name/bits/pubs/maxent13-convex-gefs.pdf">convex dual interpretations and generalisations of exponental families</a> and some similar ideas were used to get a new perspective on fast rates in online learning in our <a href="http://jmlr.org/proceedings/papers/v40/Reid15.html">COLT paper</a> this year with <a href="http://users.cecs.anu.edu.au/~nmehta/">Nishant</a> and <a href="http://users.cecs.anu.edu.au/~williams/">Bob</a>. One thing I’d like to understand better is the connection between universal barriers and what we call “entropic duals”, which I think coincide in the case of Shannon entropy. However, we show that fast rates in online prediction with expert advice can be obtained for losses satisfying a mixability condition defined in terms any Legendre function defined on convex bodies (what we call “generalised entropies”). I’d be curious to see whether there are similar implications for OLO bandit games.</p>
<p>Also in the “things I’d like to understand better” bucket is the connection between our generalised Aggregating Algorithm and the results <a href="http://people.cecs.anu.edu.au/user/5197">Kamal</a>, <a href="http://users.cecs.anu.edu.au/~williams/">Bob</a>, and <a href="http://users.cecs.anu.edu.au/~xzhang/">Xinhua</a> presented on their characterisation of <a href="http://jmlr.org/proceedings/papers/v40/Kamalaruban15.html">Exp-concave proper losses</a> and its relationship to mixability. From my brief discussions with them it seems that mixability and exp-concavity are effective the same condition, it’s just that the latter is a parameterisation-dependent version of the former.</p>
<p>It was good to see a number of other papers that looked at proper losses/scores and property elicitation, including:</p>
<ul>
<li><p><em><a href="http://jmlr.org/proceedings/papers/v40/Agarwal15.html">On Consistent Surrogte Risk Minimization and Property Elicitation</a></em> by <a href="http://clweb.csa.iisc.ernet.in/cse12/arpit.agarwal/">Arpit Agarwal</a> and <a href="http://www.shivani-agarwal.net">Shivani Agarwal</a>.</p></li>
<li><p><em><a href="http://jmlr.org/proceedings/papers/v40/Frongillo15.html">Vector-Valued Property Elicitation</a></em> by <a href="http://people.seas.harvard.edu/~raf/">Raf</a> and <a href="http://research.microsoft.com/en-us/people/iankash/">Ian Kash</a>.</p></li>
<li><p><em><a href="http://jmlr.org/proceedings/papers/v40/Telgarsky15.html">Convex Risk Minimization and Conditional Probability Estimation</a></em> by <a href="http://cseweb.ucsd.edu/~mtelgars/">Matus Telgarsky</a> <a href="http://research.microsoft.com/en-us/people/mdudik/">Miro Dudík</a> and <a href="http://research.microsoft.com/en-us/people/schapire/">Rob Schapire</a>.</p></li>
</ul>
<p>There were also a couple of other bandit-related papers that I plan to look more closely at.</p>
<p><a href="http://www.wisdom.weizmann.ac.il/~shamiro/">Ohad Shamir</a> had a nice paper <em><a href="http://jmlr.org/proceedings/papers/v40/Shamir15.html">On the Complexity of Bandit Linear Optimization</a></em> that provides some new bounds on rates for bandit games. Curiously, he shows that certain innocuous modifications that have no effect in full information games (such as translation of the action space) can adversely affect guarantees in the bandit setting.</p>
<p><a href="http://www.tau.ac.il/~nogaa/">Noga Alon</a> and co. had a follow up to some of their earlier work on graph feedback models for bandits where taking an action will reveal the rewards for neighbouring actions on a known graph. In their new paper, <em><a href="http://jmlr.org/proceedings/papers/v40/Alon15.html">Online Learning with Feedback Graphs: Beyond Bandits</a></em>, they neatly characterise three rates regimes — roughly <span class="math">\(\sqrt{T}\)</span>, <span class="math">\(T^{2/3}\)</span>, and <span class="math">\(T\)</span> — in terms of whether the feedback graph is “strongly observable” (<em>i.e.</em>, neighbours of each vertex <span class="math">\(i\)</span> include <span class="math">\(i\)</span> or all vertices except <span class="math">\(i\)</span>), “weakly observable” (<em>i.e.</em>,if all vertices have neighbours), or “unobservable” (<em>i.e.</em>, one or more vertices have no neighbours).</p>
<h2 id="wining-dining-on-a-boat">Wining &amp; Dining (on a boat!)</h2>
<p>Finally, I’d be remiss not to mention the conference events, which easily lived up to the quality of the conference content. The COLT cocktail party on top of the Zamansky tower gave us a stunning view of of Paris, as did the one hosted by Criteo at their lab. The conference dinner was also extremely scenic, cruising up and down the Seine on a floating restaurant while being treated to some delicious French food and wine.</p>
<div class="figure">
<img src="../pics/paris-sunset.jpg" alt="Parisian sunset from top of the Zamansky tower" /><p class="caption">Parisian sunset from top of the Zamansky tower</p>
</div>
<p>All in all, this was a fantastic COLT — easily one of the best I’ve been to. Congratulations and thanks to <a href="http://homepages.cwi.nl/~pdg/">Peter</a>, <a href="http://www.cs.princeton.edu/~ehazan/">Elad</a>, and <a href="https://sites.google.com/site/vianneyperchet/">Vianney</a> for a wonderful job organising it.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>A notable coincidence (at least for me) is that the one and only previous time I was in Paris in 2001 I was reading a copy of Valiant’s <em>Circuits of the Mind</em> that I’d picked up in New York. Strangely, I hadn’t really seen much referencing that work in the in the interim.<a href="#fnref1">↩</a></p></li>
</ol>
</div>


<address class="signature">
	<a class="author" href="http://mark.reid.name">Mark Reid</a>
  <span class="date">July  7, 2015</span>
  <span class="location">Paris, France</span>
</address>

</div>
<address id="feed" class="quiet right">Subscribe: <a href="../blog/atom.xml" title="Subscribe to Atom feed"><em>Atom Feed</em></a></address>


<!-- Disqus Comments -->
<div id="disqus_thread"></div>

<!-- Enable Disqus comments -->
<script type="text/javascript">
        var disqus_iframe_css = "http://mark.reid.name/css/screen.css";
        var disqus_title = "COLT 2015 in Review";
        var disqus_message = "Despite the jet-lag and the extreme heat, I had a very enjoyable time at COLT this year. This is a summary of some of the highlights for me as well as a list of work I saw that I'd like to investigate further.";
</script>
<script type="text/javascript" src="http://disqus.com/forums/markreid/embed.js"></script>

<noscript>
    <a href="http://markreid.disqus.com/?url=ref">View the discussion thread.</a>
</noscript>




    <!-- Footer with copyright and contact information -->
  <div id="footer">
	<address>
		<span class="copyright">
			Content &amp; Design by 
			<a href="../info/site.html">Mark Reid</a>
			<br />
			(<a rel="licence" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Some rights reserved</a>)			
		</span>
		<span class="engine">
			Powered by 
			<a href="http://jaspervdj.be/hakyll/" title="A static, minimalist, Haskell-powered CMS">Hakyll</a>
		</span>
	</address>
  </div>

</div>


<!-- Google Analytics script -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-1051817-4");
pageTracker._trackPageview();
</script>
<!--[if IE 6]>
<script type="text/javascript"> 
	/*Load jQuery if not already loaded*/ if(typeof jQuery == 'undefined'){ document.write("<script type=\"text/javascript\"   src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js\"></"+"script>"); var __noconflict = true; } 
	var IE6UPDATE_OPTIONS = {
		icons_path: "http://static.ie6update.com/hosted/ie6update/images/"
	}
</script>
<script type="text/javascript" src="http://static.ie6update.com/hosted/ie6update/ie6update.js"></script>
<![endif]-->


</body>
</html>



