<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>

   <!-- Standard site meta information -->
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>ICML and COLT Highlights ← Inductio Ex Machina ← Mark Reid</title>
   <meta name="author" content="Mark Reid" />

   <link rel="openid.server" href="http://www.myopenid.com/server" />
   <link rel="openid.delegate" href="http://mark.reid.name" />

   <link rel="start" href="../" />

   <!-- syntax highlighting CSS -->
   <link rel="stylesheet" href="../css/syntax.css" type="text/css" />

   <!-- Homepage CSS -->
   <link rel="stylesheet" href="../css/screen.css" type="text/css" media="all" />
   <link rel="stylesheet" href="../css/print.css" type="text/css" media="print" />
   <link rel="stylesheet" href="../css/widgets.css" type="text/css" media="all" />
   <link rel="stylesheet" href="../css/amjr.css" type="text/css" media="all" />

 	<meta name="twitter:card" content="summary">
    <meta name="twitter:creator" content="@mdreid">
    <meta name="twitter:site" content="@mdreid">
    <meta name="twitter:title" content="ICML and COLT Highlights">
    <meta name="twitter:description" content="I attended both ICML and COLT this year. This is an overview of what I thought were the most interesting talks.">

<!-- MathJax configuration and loading -->
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		extensions: ["tex2jax.js"],
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["\\(","\\)"] ],
			displayMath: [ ["\\[","\\]"] ],
			processEscapes: true
		},
		TeX: { equationNumbers: { autoNumber: "AMS" } },
		"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"}> </script>

</head>

<body id="Blog">

<div id="site">
  <div id="header">
<h1>
	<a href="../blog/" title="A machine learning blog">Inductio ex Machina</a>
	<span class="byline">← <a href="../">Mark Reid</a></span>
</h1>
<ul class="nav">
  <li><a class="home" href="../blog/">Home</a></li>
  <li><a class="info" href="../blog/info.html">Info</a></li>
  <li><a class="past" href="../blog/past.html">Past</a></li>
  <li><a class="kith" href="../blog/kith.html">Kith</a></li>
</ul>
</div>

<div id="page">

<h1 class="emphnext">ICML and COLT Highlights</h1>

<p>Now that I have had a much needed holiday I am feeling refreshed enough to write up some of my notes on the recent <a href="http://www.cs.mcgill.ca/~icml2009/">ICML</a> and <a href="http://www.cs.mcgill.ca/~colt2009/">COLT</a> conferences held in Montréal this year.</p>
<p>I did not get to see as much of either conference as I would have liked to as I was nursing an awful cold. Fortunately, I was able to fight it off long enough to finish and give my <a href="../iem/generalised-pinsker-inequalities.html">ICML and COLT presentations</a>. The ICML one almost didn’t happen as I had completely lost my voice the day before.</p>
<h2 id="icml">ICML</h2>
<p>One theory paper I really liked at ICML was <em><a href="http://conflate.net/icml/paper/2009/89">PAC-Bayesian Learning of Linear Classifiers</a></em> by Pascal Germain, Alexandre Lacasse, <a href="http://www2.ift.ulaval.ca/~laviolette/">François Laviolette</a> and <a href="http://www2.ift.ulaval.ca/~mmarchand/">Mario Marchand</a>. They give a general statement of the PAC-Bayes bound in terms of a convex function measuring the divergence between the true and empirical risks and, most importantly, give a remarkably simple proof using only Markov and Jensen’s inequalities.</p>
<p>Although I didn’t really follow the active learning aspects of <em><a href="http://conflate.net/icml/paper/2009/393">Learning from Measurements in Exponential Families</a></em> by <a href="http://www.eecs.berkeley.edu/~pliang/">Percy Liang</a>, <a href="http://www.cs.berkeley.edu/~klein">Dan Klein</a> and <a href="http://www.cs.berkeley.edu/~jordan">Michael Jordan</a>, I did think their introduction of <em>measurements</em> as a way of combining labels and constraints was elegant. Rather than assuming instances <em>X</em> have observed labels <em>Y</em>, they assume the labels are hidden and their values are only available implicitly through a set of aggregated measurements <em>M(X,Y)</em> over the whole data set. Various choices of the function <em>M</em> result in many existing learning problems.</p>
<h2 id="colt">COLT</h2>
<p>Of the presentations I managed to attend at COLT, there were two that I found thought-provoking.</p>
<p><em><a href="http://ttic.uchicago.edu/~shai/papers/ShalevShamirSridharanSrebro2.pdf">Learnability and Stability in the General Learning Setting</a></em> by <a href="http://ttic.uchicago.edu/~shai/">Shai Shalev-Shwartz</a>, <a href="http://www.cs.huji.ac.il/~ohads03/">Ohad Shamir</a>, <a href="http://ttic.uchicago.edu/~nati/">Nathan Srebro</a> and <a href="http://ttic.uchicago.edu/~karthik/">Karthik Sridharan</a> examined the “general learning setting” proposed by Vapnik where, instead of trying to minimising the expected cost of a function that predicts labels for instances <span class="math">\(\mathbb{E}[\ell(h(X), Y)]\)</span> we just want to find a point <span class="math">\(h\)</span> in a hypothesis space <span class="math">\(\mathcal{H}\)</span> that minimises <span class="math">\(\mathbb{E}[\ell(h, Z)]\)</span>. The relationship between hypotheses and instances in the latter case is much more arbitrary than in the usual supervised setting.</p>
<p>Interestingly, the usual correspondence between uniform convergence of empirical risks to the expected risk and learnability which holds for supervised learning with empirical risk minimisation (ERM) breaks down in the general setting. In particular, it turns out uniform convergence is sufficient for learning with ERM but not necessary. The authors therefore explore generalised notions of ERM and show that learnability and stability are equivalent for these more general versions.</p>
<p>Finally, I really liked the connections made between online learning and traditional statistical learning theory in <em><a href="http://arxiv.org/abs/0903.5328">A Stochastic View of Optimal Regret through Minimax Duality</a></em> by <a href="http://www.eecs.berkeley.edu/~jake/">Jacob Abernethy</a>, <a href="http://www.eecs.berkeley.edu/~alekh/">Alekh Agarwal</a>, <a href="http://www.stat.berkeley.edu/~bartlett/">Peter Bartlett</a> and <a href="http://www-stat.wharton.upenn.edu/~rakhlin/">Alexander Rakhlin</a>. In this paper the authors use the von Neumann’s minimax theorem to relate the minimax regret that is normally used in online convex optimisation (OCO) games to a supremum over a set of distributions of the expectations of a loss. Through a neat geometrical interpretation of this loss they are able to establish upper and lower bounds on the optimal regret for various online learning problems.</p>
<h2 id="more-highlights">More Highlights</h2>
<p>If you are after more highlights, I recommend having a look at <a href="http://hunch.net/?p=813">John</a> and <a href="http://nlpers.blogspot.com/2009/06/icmlcoltuai-2009-retrospective.html">Hal</a>’s overviews of the conferences.</p>


<address class="signature">
	<a class="author" href="http://mark.reid.name">Mark Reid</a>
  <span class="date">July  7, 2009</span>
  <span class="location">Canberra, Australia</span>
</address>

</div>
<address id="feed" class="quiet right">Subscribe: <a href="../blog/atom.xml" title="Subscribe to Atom feed"><em>Atom Feed</em></a></address>


<!-- Disqus Comments -->
<div id="disqus_thread"></div>

<!-- Enable Disqus comments -->
<script type="text/javascript">
        var disqus_iframe_css = "http://mark.reid.name/css/screen.css";
        var disqus_title = "ICML and COLT Highlights";
        var disqus_message = "I attended both ICML and COLT this year. This is an overview of what I thought were the most interesting talks.";
</script>
<script type="text/javascript" src="http://disqus.com/forums/markreid/embed.js"></script>

<noscript>
    <a href="http://markreid.disqus.com/?url=ref">View the discussion thread.</a>
</noscript>




    <!-- Footer with copyright and contact information -->
  <div id="footer">
	<address>
		<span class="copyright">
			Content &amp; Design by 
			<a href="../info/site.html">Mark Reid</a>
			<br />
			(<a rel="licence" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Some rights reserved</a>)			
		</span>
		<span class="engine">
			Powered by 
			<a href="http://jaspervdj.be/hakyll/" title="A static, minimalist, Haskell-powered CMS">Hakyll</a>
		</span>
	</address>
  </div>

</div>


<!-- Google Analytics script -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-1051817-4");
pageTracker._trackPageview();
</script>
<!--[if IE 6]>
<script type="text/javascript"> 
	/*Load jQuery if not already loaded*/ if(typeof jQuery == 'undefined'){ document.write("<script type=\"text/javascript\"   src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js\"></"+"script>"); var __noconflict = true; } 
	var IE6UPDATE_OPTIONS = {
		icons_path: "http://static.ie6update.com/hosted/ie6update/images/"
	}
</script>
<script type="text/javascript" src="http://static.ie6update.com/hosted/ie6update/ie6update.js"></script>
<![endif]-->


</body>
</html>



