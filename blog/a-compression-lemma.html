<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>

   <!-- Standard site meta information -->
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>The Compression Lemma ← Inductio Ex Machina ← Mark Reid</title>
   <meta name="author" content="Mark Reid" />

   <link rel="openid.server" href="http://www.myopenid.com/server" />
   <link rel="openid.delegate" href="http://mark.reid.name" />

   <link rel="start" href="../" />

   <!-- syntax highlighting CSS -->
   <link rel="stylesheet" href="../css/syntax.css" type="text/css" />

   <!-- Homepage CSS -->
   <link rel="stylesheet" href="../css/screen.css" type="text/css" media="all" />
   <link rel="stylesheet" href="../css/print.css" type="text/css" media="print" />
   <link rel="stylesheet" href="../css/widgets.css" type="text/css" media="all" />
   <link rel="stylesheet" href="../css/amjr.css" type="text/css" media="all" />

 	<meta name="twitter:card" content="summary">
    <meta name="twitter:creator" content="@mdreid">
    <meta name="twitter:site" content="@mdreid">
    <meta name="twitter:title" content="The Compression Lemma">
    <meta name="twitter:description" content="A brief discussion and proof of this very elegant and powerful result of Banerjee's.">

<!-- MathJax configuration and loading -->
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		extensions: ["tex2jax.js"],
		jax: ["input/TeX", "output/HTML-CSS"],
		tex2jax: {
			inlineMath: [ ["\\(","\\)"] ],
			displayMath: [ ["\\[","\\]"] ],
			processEscapes: true
		},
		TeX: { equationNumbers: { autoNumber: "AMS" } },
		"HTML-CSS": { availableFonts: ["TeX"] }
	});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>

</head>

<body id="Blog">

<div id="site">
  <div id="header">
<h1>
	<a href="../blog/" title="A machine learning blog">Inductio ex Machina</a>
	<span class="byline">← <a href="../">Mark Reid</a></span>
</h1>
<ul class="nav">
  <li><a class="home" href="../blog/">Home</a></li>
  <li><a class="info" href="../blog/info.html">Info</a></li>
  <li><a class="past" href="../blog/past.html">Past</a></li>
  <li><a class="kith" href="../blog/kith.html">Kith</a></li>
</ul>
</div>

<div id="page">

<h1 class="emphnext">The Compression Lemma</h1>

<p>I recently re-read an ICML 2006 paper called <em><a href="http://www-users.cs.umn.edu/~banerjee/papers/06/icml06fenchel.pdf">On Bayesian Bounds</a></em> by <a href="http://www-users.cs.umn.edu/~banerjee/">Arindam Banerjee</a> that has at its core a very elegant and general result. I spent a bit of time trying to carefully understand it and thought I would write up my take on it. Here’s a slight restatement of the key result.</p>
<blockquote>
<p><strong>Compression Lemma</strong> (Banerjee, 2006)<br />For any function <span class="math">\(\phi : X \to \mathbb{R}\)</span> and any pair of distributions <span class="math">\(P\)</span> and <span class="math">\(Q\)</span> over <span class="math">\(X\)</span> we have <span class="math">\[ 
	\mathbb{E}_Q\left[\phi(x)\right] - \log \mathbb{E}_P\left[e^{\phi(x)}\right]
	\le KL(Q\|P)
\]</span> where <span class="math">\(KL(Q\|P) = \mathbb{E}_Q\left[\log\frac{dQ}{dP}\right]\)</span> is the <a href="http://en.wikipedia.org/wiki/Kullback–Leibler_divergence">Kulback-Leibler divergence</a> from <span class="math">\(P\)</span> to <span class="math">\(Q\)</span>. Furthermore, the bound is achieved for <span class="math">\(\phi(x) = \log\frac{dQ}{dP}(x)\)</span>.</p>
</blockquote>
<p>Notice that there are no restrictions placed on the choice of the function (aside from implicit measurability) or the distributions. The fact that this inequality holds so broadly is the reason Banerjee is able to use it to derive a <a href="http://videolectures.net/aop07_shawe_taylor_pba/">PAC-Bayesian bound</a> and other related bounds in online learning. I won’t go into these here but it is worth looking at the paper to see how easily these are derived through a judicious choice of <span class="math">\(\phi\)</span>.</p>
<p>The reason for the name <em>compression lemma</em> is not apparent at first. Indeed, apart from the presence of the KL divergence, there is little to immediately connect it with information theory. Fortunately, the paper does give a way of looking at it that makes this connection clear.</p>
<p>What follows is my own simplification of the proof given by Banerjee. It uses all the same ideas but simplifies his argument a little.</p>
<h2 id="a-simple-proof">A Simple Proof</h2>
<p>We need to introduce a slight variant of the KL divergence which, for wont of a better name, I’ll call the <em>relative <a href="http://en.wikipedia.org/wiki/Cross_entropy">cross entropy</a></em> of distributions <span class="math">\(Q\)</span> and <span class="math">\(R\)</span> relative to <span class="math">\(P\)</span>: <span class="math">\[ 
	C(Q,R\|P) = \mathbb{E}_Q\left[\log \frac{dR}{dP} \right].
\]</span> The negative relative cross entropy can be interpreted as the expected code length required to encode symbols drawn from <span class="math">\(Q\)</span> when using a code based on <span class="math">\(R\)</span>.</p>
<p>As you can see, the relative cross entropy is equal to the KL divergence when <span class="math">\(R=Q\)</span>. In fact, we can easily see that <span class="math">\[ 
	C(Q,R\|P) = KL(Q\|P) - KL(Q\|R)
\]</span> by noting that <span class="math">\(\log \frac{dR}{dP} = \log\left( \frac{dR}{dQ}\ \frac{dQ}{dP} \right) = -\log \frac{dQ}{dR} + \log \frac{dQ}{dP}\)</span> and splitting and matching the resulting expectation with the definition of KL divergence.</p>
<p>Since the KL divergence is guaranteed to be non-negative, we see immediately that <span class="math">\(C(Q,R\|P) \le KL(Q\|P)\)</span> with equality holding when <span class="math">\(KL(Q\|R) = 0\)</span>, that is, when <span class="math">\(R=Q\)</span>.</p>
<p>Now we consider a specific choice of <span class="math">\(R\)</span>, namely a <a href="http://en.wikipedia.org/wiki/Gibbs_measure">Gibbs distribution</a> based on the function <span class="math">\(\phi\)</span>. Its density relative to <span class="math">\(P\)</span> is given by <span class="math">\[ 
	\frac{dR}{dP}(x) = \frac{1}{Z_{\phi}} e^{\phi(x)} 
\]</span> where <span class="math">\(Z_{\phi} = \mathbb{E}_P\left[ \exp(\phi(x)) \right]\)</span> is just a normalising term.</p>
<p>Plugging this definition of <span class="math">\(R\)</span> into <span class="math">\(C(Q,R\|P)\)</span> gives <span class="math">\[ 
	\mathbb{E}_Q\left[\log\left(Z_{\phi}^{-1}e^{\phi(x)}\right)\right]
	= \mathbb{E}_Q\left[\phi(x)\right] - \log \mathbb{E}_P\left[ e^{\phi(x)} \right]
\]</span> and since <span class="math">\(C(Q,R\|P) \le KL(Q\|P)\)</span> we have proved the first part of the compression lemma. The second part follows by substituting <span class="math">\(\phi(x) = \log \frac{dQ}{dP}(x)\)</span> into the definition of the Gibbs distribution <span class="math">\(R\)</span> and noting that, in this case, <span class="math">\(R = Q\)</span>.</p>
<h2 id="some-observations">Some Observations</h2>
<p>Banerjee makes a number of insightful observations about the compression lemma, including its connection with the class of tight bounds achievable via <a href="http://en.wikipedia.org/wiki/Fenchel's_duality_theorem">Fenchel duality</a>.</p>
<p>One small thing I think my proof adds is a simple quantification of the gap in the compression lemma inequality. Specifically, it is <span class="math">\(KL(Q\|R)\)</span>—the divergence of <span class="math">\(Q\)</span>, the distribution generating the data, from <span class="math">\(R\)</span>, the distribution modelling <span class="math">\(Q\)</span>. While this doesn’t have a simple form when <span class="math">\(R\)</span> is a Gibbs distribution for <span class="math">\(\phi\)</span> (at least I can’t see it) it has straight-forward interpretation.</p>


<address class="signature">
	<a class="author" href="http://mark.reid.name">Mark Reid</a>
  <span class="date">August 16, 2010</span>
  <span class="location">Canberra, Australia</span>
</address>

</div>
<address id="feed" class="quiet right">Subscribe: <a href="../blog/atom.xml" title="Subscribe to Atom feed"><em>Atom Feed</em></a></address>


<!-- Disqus Comments -->
<div id="disqus_thread"></div>

<!-- Enable Disqus comments -->
<script type="text/javascript">
        var disqus_iframe_css = "http://mark.reid.name/css/screen.css";
        var disqus_title = "The Compression Lemma";
        var disqus_message = "A brief discussion and proof of this very elegant and powerful result of Banerjee's.";
</script>
<script type="text/javascript" src="http://disqus.com/forums/markreid/embed.js"></script>

<noscript>
    <a href="http://markreid.disqus.com/?url=ref">View the discussion thread.</a>
</noscript>




    <!-- Footer with copyright and contact information -->
  <div id="footer">
	<address>
		<span class="copyright">
			Content &amp; Design by 
			<a href="../info/site.html">Mark Reid</a>
			<br />
			(<a rel="licence" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Some rights reserved</a>)			
		</span>
		<span class="engine">
			Powered by 
			<a href="http://jaspervdj.be/hakyll/" title="A static, minimalist, Haskell-powered CMS">Hakyll</a>
		</span>
	</address>
  </div>

</div>


<!-- Google Analytics script -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-1051817-4");
pageTracker._trackPageview();
</script>
<!--[if IE 6]>
<script type="text/javascript"> 
	/*Load jQuery if not already loaded*/ if(typeof jQuery == 'undefined'){ document.write("<script type=\"text/javascript\"   src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js\"></"+"script>"); var __noconflict = true; } 
	var IE6UPDATE_OPTIONS = {
		icons_path: "http://static.ie6update.com/hosted/ie6update/images/"
	}
</script>
<script type="text/javascript" src="http://static.ie6update.com/hosted/ie6update/ie6update.js"></script>
<![endif]-->


</body>
</html>



